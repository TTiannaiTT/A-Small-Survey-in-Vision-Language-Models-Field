# New Research Trends in the Vision-Language Models Field: A Small Survey

New Research Trends in the Vision-Language Models Field: A Small Survey (2024), for Machine Learning Practice (2024 Fall) assignment 2.

## News
- **2024-11-02** : Create this repo.

## Paper List
1. **Gradient-based learning applied to document recognition**: Y. Lecun; L. Bottou; Y. Bengio; P. Haffner, 1998. [Link](https://ieeexplore.ieee.org/document/726791)
2. **Bidirectional recurrent neural networks**: M. Schuster; K.K. Paliwal, 1998. [Link](https://ieeexplore.ieee.org/document/650093)
3. **Attention is All you Need**: Ashish Vaswani, Noam M. Shazeer, Illia Polosukhin, Year. [2017]([Link](https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776)
4. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**:Jacob Devlin, Ming-Wei Chang, Kristina Toutanova, 2019. [Link](https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992)
5. **Gradient-based learning applied to document recognition**: Y. Lecun; L. Bottou; Y. Bengio; P. Haffner, 1998. [Link](https://ieeexplore.ieee.org/document/726791)
6. **Learning Transferable Visual Models From Natural Language Supervision**: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, 2021. [Link](https://arxiv.org/abs/2103.00020)
7. **Unifying Visual and Vision-Language Tracking via Contrastive Learning**: Yinchao Ma, Yuyang Tang, Wenfei Yang, Tianzhu Zhang, Jinpeng Zhang, Mengxue Kang, 2024. [Link](https://arxiv.org/abs/2401.11228)
8. **Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision**:Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig, 2021. [Link](https://arxiv.org/abs/2102.05918)
9. **Attention is All you Need**: Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig, 2021. [Link](https://arxiv.org/abs/2102.05918)
10. **Efficient Vision-Language Pre-training by Cluster Masking**: Zihao Wei, Zixuan Pan, Andrew Owens, 2024. [Link](https://arxiv.org/abs/2405.08815)
11. **BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation**: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, 2022. [Link](https://arxiv.org/abs/2201.12086)
13. **VILA: On Pre-training for Visual Language Models**: Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han, 2024. [Link](https://arxiv.org/abs/2312.07533))
14. **Zero-Shot Text-to-Image Generation**: Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, 2021. [Link](https://arxiv.org/abs/2102.12092))
15. **RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness**: Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun, 2024. [Link](https://arxiv.org/abs/2405.17220))
16. **Visual In-Context Prompting**: Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, Jianfeng Gao, 2023. [Link](https://arxiv.org/abs/2311.13601))
17. **CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts**: Yichao Cai, Yuhang Liu, Zhen Zhang, Javen Qinfeng Shi, 2024. [Link](https://arxiv.org/abs/2311.16445))
18. **CLIP-Adapter: Better Vision-Language Models with Feature Adapters**: Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao, 2021. [Link](https://arxiv.org/abs/2110.04544)
19. **Language Models as Black-Box Optimizers for Vision-Language Models**: Shihong Liu, Zhiqiu Lin, Samuel Yu, Ryan Lee, Tiffany Ling, Deepak Pathak, Deva Ramanan, 2023. [Link](https://arxiv.org/abs/2309.05950))

## Dataset
- **Densely Captioned Images (DCI)**: 包含8012张自然图像，每张图像都有超过1000字的详细描述. [Link](https://gitcode.com/gh_mirrors/dc/DCI)
- **COCO (Common Objects in Context)**: 一个广泛用于图像识别、分割和字幕生成的数据集，包含超过90万张图像，每张图像都配有多个描述性的标签和注释. [Link](http://cocodataset.org)
- **Flickr30k**: 一个图像-文本配对数据集，包含超过3万张图像和相应的描述性句子，常用于图像-文本匹配和字幕生成任务. [Link](https://shannon.cs.illinois.edu/DenotationGraph/)

## Reference
This work is based on and extends the survey presented in [Vision-Language Models for Vision Tasks: A Survey](https://arxiv.org/abs/2304.00685) and its [repo](https://github.com/jingyi0000/VLM_survey).
## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
